{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMNIST_DNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN7kNn/AVSPyp/YgCihrWdx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodychang0611/EMNIST/blob/master/EMNIST_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6xodRU-A_GV",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Environment\n",
        "If running on colab mount google drive, otherwise use 200g drive from NCCU GPU cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqMs0yceAxaw",
        "colab_type": "code",
        "outputId": "c8a52de1-0681-4a52-ccd0-61932d8ef6c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import sys\n",
        "import os\n",
        "if ('google.colab' in sys.modules):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  gdrive_root = 'gdrive/My Drive/Deep_Learning/'\n",
        "  dataset_path = os.path.join(gdrive_root,'Dataset')\n",
        "else:\n",
        "  dataset_path = '200g/Dataset'\n",
        "  pass\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "  raise Exception(f'dataset_path \"{dataset_path}\"\" does not exist')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOQv68AZC_M4",
        "colab_type": "text"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ9n-431460N",
        "colab_type": "code",
        "outputId": "4509cfb9-c421-4ddb-efb6-fde077a0b6d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.autograd\n",
        "import torchvision\n",
        "import torchvision.transforms\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import datetime\n",
        "\n",
        "trans = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5,), (1.0,))])\n",
        "use_cuda = None\n",
        "if torch.cuda.is_available():\n",
        "  print(\"CUDA available\")\n",
        "  use_cuda = True\n",
        "else:\n",
        "  print (\"CUDA not available\")\n",
        "  use_cuda = False\n",
        "if os.path.exists(dataset_path):\n",
        "  train_set = torchvision.datasets.EMNIST(root=dataset_path, transform=trans,train =True, split=\"byclass\",download=True)\n",
        "  test_set = torchvision.datasets.EMNIST(root=dataset_path, transform=trans,split=\"byclass\", train =False)\n",
        "  print (f'Dataset loaded, {train_set.__len__():,} training set, {test_set.__len__():,} testing set')\n",
        "else:\n",
        "  print (f'dataset_path \"{dataset_path}\" not found')\n",
        "  exit(0)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA available\n",
            "Dataset loaded, 697,932 training set, 116,323 testing set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIKW9C5mbBXs",
        "colab_type": "text"
      },
      "source": [
        "# Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXFaOXpTbDsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ceriation = nn.CrossEntropyLoss()\n",
        "def apply_model(model,x,target):\n",
        "    x, target = torch.autograd.Variable(x), torch.autograd.Variable(target)\n",
        "    out = model(x)\n",
        "    loss = torch.nn.CrossEntropyLoss()(out, target)\n",
        "    return out,loss\n",
        "def plot_graph(name,train_loss,test_loss):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(train_loss,label='train loss')\n",
        "  ax.plot(test_loss,label='test loss')  \n",
        "  ax.set(xlabel='epoch', ylabel='Loss',title=name)\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojpFdoTxa3WU",
        "colab_type": "text"
      },
      "source": [
        "# Define DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD_XLlL2-keO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DNN\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self,dropouts=(0,),layer_widths=(512,)):\n",
        "        super(DNN, self).__init__()\n",
        "        if (len(dropouts) != len(layer_widths) or len(dropouts)<=0 or len(layer_widths)<=0):\n",
        "          raise Exception (\"dropouts and layer_widths must have same width and greater than 0\")\n",
        "        self.dropouts=dropouts\n",
        "        self.layer_widths = layer_widths\n",
        "        input_width = 28*28\n",
        "        self.fc = nn.ModuleList()\n",
        "        for width in self.layer_widths:\n",
        "          self.fc.append(nn.Linear(input_width, width))\n",
        "          input_width = width\n",
        "        self.final = nn.Linear(layer_widths[-1], 62)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        for dropout, fc in zip(self.dropouts,self.fc):\n",
        "          x = F.dropout(x,dropout)\n",
        "          x = fc(x)\n",
        "          x = F.relu(x)\n",
        "        x = self.final(x)\n",
        "        return x\n",
        "    @property\n",
        "    def name(self):\n",
        "        return f\"DNN, Width:{self.layer_widths}, Dropout: {self.dropouts}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceS6a8HGuFjp",
        "colab_type": "code",
        "outputId": "5aa15bea-97d5-48c8-82bf-18d2cd5df2d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "models = (\n",
        "    DNN(layer_widths = (500,256,), dropouts = (0,0,)),    \n",
        "    DNN(layer_widths = (700,256,), dropouts = (0,0,)),  \n",
        "    DNN(layer_widths = (700,500,256,), dropouts = (0,0,0,)),\n",
        "    DNN(layer_widths = (700,500,256,), dropouts = (0.2,0,0,)), \n",
        "    )\n",
        "sample_limit = 100000000\n",
        "for model in models:\n",
        "  name = model.name\n",
        "  print(datetime.datetime.now())\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "  parameter_len = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f'model:{name}, parameter:{parameter_len:,}')\n",
        "  train_loss=[]\n",
        "  test_loss=[]\n",
        "  if use_cuda:\n",
        "    model.cuda()\n",
        "  for epoch in range(25):\n",
        "    # training\n",
        "    ave_loss = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        out,loss = apply_model (model,x,target)\n",
        "        ave_loss = ave_loss * 0.9 + loss.data* 0.1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (batch_idx+1) == len(train_loader) or batch_idx >sample_limit:\n",
        "            print (f'==>>> epoch: {epoch}, batch index: {batch_idx+1}, train loss: {ave_loss:.6f}')\n",
        "            break\n",
        "    train_loss.append(ave_loss)\n",
        "    # testing\n",
        "    correct_cnt, ave_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        if use_cuda:\n",
        "          x, target = x.cuda(), target.cuda()      \n",
        "        out,loss = apply_model (model,x,target)        \n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth average\n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        if (batch_idx) == len(test_loader)-1 or batch_idx >sample_limit:\n",
        "            print (f'==>>> epoch: {epoch}, batch index: {batch_idx+1}, test loss: {ave_loss:.6f}, acc: {correct_cnt * 1.0 / total_cnt:.3f}')\n",
        "            break\n",
        "    test_loss.append(ave_loss)\n",
        "  plot_graph(name,train_loss,test_loss)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-04 04:55:38.783512\n",
            "model:DNN, Width:(500, 256), Dropout: (0, 0), parameter:536,690\n",
            "==>>> epoch: 0, batch index: 2727, train loss: 0.737236\n",
            "==>>> epoch: 0, batch index: 455, test loss: 0.699299, acc: 0.784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyANcMBjstan",
        "colab_type": "text"
      },
      "source": [
        "model:DNN, Width:700, Dropout: 0, batch_normalization: False, parameter:746,802\n",
        "\n",
        "==>>> epoch: 16, batch index: 909, test loss: 0.390396, acc: 0.858"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9umS6l0VI-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "2020-03-31 11:03:30.496378\n",
        "model:Baseline Net, parameter:536,690\n",
        "==>>> epoch: 0, batch index: 5453, train loss: 0.609612\n",
        "==>>> epoch: 0, batch index: 909, test loss: 0.585146, acc: 0.809\n",
        "==>>> epoch: 1, batch index: 5453, train loss: 0.540112\n",
        "==>>> epoch: 1, batch index: 909, test loss: 0.493715, acc: 0.827\n",
        "==>>> epoch: 2, batch index: 5453, train loss: 0.454728\n",
        "==>>> epoch: 2, batch index: 909, test loss: 0.459707, acc: 0.839\n",
        "==>>> epoch: 3, batch index: 5453, train loss: 0.453971\n",
        "==>>> epoch: 3, batch index: 909, test loss: 0.450110, acc: 0.843\n",
        "==>>> epoch: 4, batch index: 5453, train loss: 0.449814\n",
        "==>>> epoch: 4, batch index: 909, test loss: 0.446976, acc: 0.844\n",
        "==>>> epoch: 5, batch index: 5453, train loss: 0.424157\n",
        "==>>> epoch: 5, batch index: 909, test loss: 0.423205, acc: 0.847\n",
        "==>>> epoch: 6, batch index: 5453, train loss: 0.400469\n",
        "==>>> epoch: 6, batch index: 909, test loss: 0.418983, acc: 0.850\n",
        "==>>> epoch: 7, batch index: 5453, train loss: 0.387716\n",
        "==>>> epoch: 7, batch index: 909, test loss: 0.417940, acc: 0.852\n",
        "==>>> epoch: 8, batch index: 5453, train loss: 0.356984\n",
        "==>>> epoch: 8, batch index: 909, test loss: 0.400661, acc: 0.854\n",
        "==>>> epoch: 9, batch index: 5453, train loss: 0.410329\n",
        "==>>> epoch: 9, batch index: 909, test loss: 0.411788, acc: 0.852\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}