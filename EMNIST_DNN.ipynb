{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMNIST_DNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNUUXLhXFY+z66kenS5jOzw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodychang0611/EMNIST/blob/master/EMNIST_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6xodRU-A_GV",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Environment\n",
        "If running on colab mount google drive, otherwise use 200g drive from NCCU GPU cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqMs0yceAxaw",
        "colab_type": "code",
        "outputId": "ad00664a-024b-4249-90bc-81dc7b9f2828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import sys\n",
        "import os\n",
        "if ('google.colab' in sys.modules):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  gdrive_root = 'gdrive/My Drive/Deep_Learning/'\n",
        "  dataset_path = os.path.join(gdrive_root,'Dataset')\n",
        "else:\n",
        "  dataset_path = '200g/Dataset'\n",
        "  pass\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "  raise Exception(f'dataset_path \"{dataset_path}\"\" does not exist')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOQv68AZC_M4",
        "colab_type": "text"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ9n-431460N",
        "colab_type": "code",
        "outputId": "5a72adf8-ea81-4991-cdfe-a1fdc529bb96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.autograd\n",
        "import torchvision\n",
        "import torchvision.transforms\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import datetime\n",
        "\n",
        "trans = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5,), (1.0,))])\n",
        "use_cuda = None\n",
        "if torch.cuda.is_available():\n",
        "  print(\"CUDA available\")\n",
        "  use_cuda = True\n",
        "else:\n",
        "  print (\"CUDA not available\")\n",
        "  use_cuda = False\n",
        "if os.path.exists(dataset_path):\n",
        "  train_set = torchvision.datasets.EMNIST(root=dataset_path, transform=trans,train =True, split=\"byclass\",download=True)\n",
        "  test_set = torchvision.datasets.EMNIST(root=dataset_path, transform=trans,split=\"byclass\", train =False)\n",
        "  print (f'Dataset loaded, {train_set.__len__():,} training set, {test_set.__len__():,} testing set')\n",
        "else:\n",
        "  print (f'dataset_path \"{dataset_path}\" not found')\n",
        "  exit(0)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA available\n",
            "Dataset loaded, 697,932 training set, 116,323 testing set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIKW9C5mbBXs",
        "colab_type": "text"
      },
      "source": [
        "# Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXFaOXpTbDsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ceriation = nn.CrossEntropyLoss()\n",
        "def apply_model(model,x,target):\n",
        "    x, target = torch.autograd.Variable(x), torch.autograd.Variable(target)\n",
        "    out = model(x)\n",
        "    loss = torch.nn.CrossEntropyLoss()(out, target)\n",
        "    return out,loss\n",
        "def plot_graph(name,train_loss,test_loss):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(train_loss,label='train loss')\n",
        "  ax.plot(test_loss,label='test loss')  \n",
        "  ax.set(xlabel='epoch', ylabel='Loss',title=name)\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojpFdoTxa3WU",
        "colab_type": "text"
      },
      "source": [
        "# Define DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD_XLlL2-keO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DNN\n",
        "class BaselineNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 500)\n",
        "        self.fc2 = nn.Linear(500, 256)\n",
        "        self.fc3 = nn.Linear(256, 62)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "class ImprovedeNet(nn.Module):\n",
        "    def __init__(self,dropout=0,batch_normalization=False,width=512):\n",
        "        super(ImprovedeNet, self).__init__()\n",
        "        self.dropout=dropout\n",
        "        self.batch_normalization=batch_normalization\n",
        "        self.fc1 = nn.Linear(28*28, width)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=width)\n",
        "        self.fc2 = nn.Linear(width, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=256)\n",
        "        self.fc3 = nn.Linear(256, 62)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.dropout(x,self.dropout)\n",
        "        x = self.fc1(x)\n",
        "        if self.batch_normalization:\n",
        "            x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        if self.batch_normalization:\n",
        "            x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x,self.dropout)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceS6a8HGuFjp",
        "colab_type": "code",
        "outputId": "d54a1454-e1de-4b60-bb6d-62199b8ec1cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "models = (\n",
        "    (\"Improved Net width =700\",ImprovedeNet(width = 700, batch_normalization=False)),\n",
        "    (\"Improved Net width = 1000\",ImprovedeNet(width=1000, batch_normalization=False)),    \n",
        "    )\n",
        "sample_limit = 100000000\n",
        "for (name, model) in models:\n",
        "  print(datetime.datetime.now())\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "  parameter_len = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f'model:{name}, parameter:{parameter_len:,}')\n",
        "  train_loss=[]\n",
        "  test_loss=[]\n",
        "  if use_cuda:\n",
        "    model.cuda()\n",
        "  for epoch in range(30):\n",
        "    # training\n",
        "    ave_loss = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        out,loss = apply_model (model,x,target)\n",
        "        ave_loss = ave_loss * 0.9 + loss.data* 0.1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (batch_idx+1) == len(train_loader) or batch_idx >sample_limit:\n",
        "            print (f'==>>> epoch: {epoch}, batch index: {batch_idx+1}, train loss: {ave_loss:.6f}')\n",
        "            break\n",
        "    train_loss.append(ave_loss)\n",
        "    # testing\n",
        "    correct_cnt, ave_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        if use_cuda:\n",
        "          x, target = x.cuda(), target.cuda()      \n",
        "        out,loss = apply_model (model,x,target)        \n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth average\n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        if (batch_idx) == len(test_loader)-1 or batch_idx >sample_limit:\n",
        "            print (f'==>>> epoch: {epoch}, batch index: {batch_idx+1}, test loss: {ave_loss:.6f}, acc: {correct_cnt * 1.0 / total_cnt:.3f}')\n",
        "            break\n",
        "    test_loss.append(ave_loss)\n",
        "  plot_graph(name,train_loss,test_loss)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-31 13:26:25.864519\n",
            "model:Improved Net width =700, parameter:746,802\n",
            "==>>> epoch: 0, batch index: 5453, train loss: 0.635100\n",
            "==>>> epoch: 0, batch index: 909, test loss: 0.572262, acc: 0.809\n",
            "==>>> epoch: 1, batch index: 5453, train loss: 0.482484\n",
            "==>>> epoch: 1, batch index: 909, test loss: 0.483241, acc: 0.831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9umS6l0VI-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "744ef5ea-ce27-465c-ad98-595190bb8884"
      },
      "source": [
        "2020-03-31 11:03:30.496378\n",
        "model:Baseline Net, parameter:536,690\n",
        "==>>> epoch: 0, batch index: 5453, train loss: 0.609612\n",
        "==>>> epoch: 0, batch index: 909, test loss: 0.585146, acc: 0.809\n",
        "==>>> epoch: 1, batch index: 5453, train loss: 0.540112\n",
        "==>>> epoch: 1, batch index: 909, test loss: 0.493715, acc: 0.827\n",
        "==>>> epoch: 2, batch index: 5453, train loss: 0.454728\n",
        "==>>> epoch: 2, batch index: 909, test loss: 0.459707, acc: 0.839\n",
        "==>>> epoch: 3, batch index: 5453, train loss: 0.453971\n",
        "==>>> epoch: 3, batch index: 909, test loss: 0.450110, acc: 0.843\n",
        "==>>> epoch: 4, batch index: 5453, train loss: 0.449814\n",
        "==>>> epoch: 4, batch index: 909, test loss: 0.446976, acc: 0.844\n",
        "==>>> epoch: 5, batch index: 5453, train loss: 0.424157\n",
        "==>>> epoch: 5, batch index: 909, test loss: 0.423205, acc: 0.847\n",
        "==>>> epoch: 6, batch index: 5453, train loss: 0.400469\n",
        "==>>> epoch: 6, batch index: 909, test loss: 0.418983, acc: 0.850\n",
        "==>>> epoch: 7, batch index: 5453, train loss: 0.387716\n",
        "==>>> epoch: 7, batch index: 909, test loss: 0.417940, acc: 0.852\n",
        "==>>> epoch: 8, batch index: 5453, train loss: 0.356984\n",
        "==>>> epoch: 8, batch index: 909, test loss: 0.400661, acc: 0.854\n",
        "==>>> epoch: 9, batch index: 5453, train loss: 0.410329\n",
        "==>>> epoch: 9, batch index: 909, test loss: 0.411788, acc: 0.852\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-0c0c7ee4f6ea>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2020-03-31 11:03:30.496378\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid token\n"
          ]
        }
      ]
    }
  ]
}